{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Grid Weather Station\n",
    "Find the nearest grid weather station to each air statioin based on the geo_location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x,y):\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "df_air_info = pd.read_csv('Air_Station_ID.csv')\n",
    "df_grid_info = pd.read_csv('Beijing_grid_weather_station.csv')\n",
    "\n",
    "no_air = df_air_info['stationId'].count()\n",
    "no_grid = df_grid_info['gridId'].count()\n",
    "\n",
    "air_join_grid = []\n",
    "for i in range(no_air):\n",
    "    temp = []\n",
    "    air_geo = np.array([df_air_info['long'].iloc[i],df_air_info['lat'].iloc[i]])\n",
    "    for j in range(no_grid):\n",
    "        grid_geo = np.array([df_grid_info['long'].iloc[j],df_grid_info['lat'].iloc[j]])\n",
    "        distance = euclidean_distance(air_geo,grid_geo)\n",
    "        temp.append([df_grid_info['gridId'].iloc[j],distance])\n",
    "    nearest_index = np.argmin(np.array(temp),axis = 0)[1]\n",
    "    air_join_grid.append([df_air_info['stationId'].iloc[i], df_grid_info['gridId'].iloc[nearest_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dongsi_aq', 'beijing_grid_303'],\n",
       " ['tiantan_aq', 'beijing_grid_303'],\n",
       " ['guanyuan_aq', 'beijing_grid_282'],\n",
       " ['wanshouxigong_aq', 'beijing_grid_303'],\n",
       " ['aotizhongxin_aq', 'beijing_grid_304'],\n",
       " ['nongzhanguan_aq', 'beijing_grid_324'],\n",
       " ['wanliu_aq', 'beijing_grid_283'],\n",
       " ['beibuxinqu_aq', 'beijing_grid_263'],\n",
       " ['zhiwuyuan_aq', 'beijing_grid_262'],\n",
       " ['fengtaihuayuan_aq', 'beijing_grid_282'],\n",
       " ['yungang_aq', 'beijing_grid_239'],\n",
       " ['gucheng_aq', 'beijing_grid_261'],\n",
       " ['fangshan_aq', 'beijing_grid_238'],\n",
       " ['daxing_aq', 'beijing_grid_301'],\n",
       " ['yizhuang_aq', 'beijing_grid_323'],\n",
       " ['tongzhou_aq', 'beijing_grid_366'],\n",
       " ['shunyi_aq', 'beijing_grid_368'],\n",
       " ['pingchang_aq', 'beijing_grid_264'],\n",
       " ['mentougou_aq', 'beijing_grid_240'],\n",
       " ['pinggu_aq', 'beijing_grid_452'],\n",
       " ['huairou_aq', 'beijing_grid_349'],\n",
       " ['miyun_aq', 'beijing_grid_392'],\n",
       " ['yanqin_aq', 'beijing_grid_225'],\n",
       " ['dingling_aq', 'beijing_grid_265'],\n",
       " ['badaling_aq', 'beijing_grid_224'],\n",
       " ['miyunshuiku_aq', 'beijing_grid_414'],\n",
       " ['donggaocun_aq', 'beijing_grid_452'],\n",
       " ['yongledian_aq', 'beijing_grid_385'],\n",
       " ['yufa_aq', 'beijing_grid_278'],\n",
       " ['liulihe_aq', 'beijing_grid_216'],\n",
       " ['qianmen_aq', 'beijing_grid_303'],\n",
       " ['yongdingmennei_aq', 'beijing_grid_303'],\n",
       " ['xizhimenbei_aq', 'beijing_grid_283'],\n",
       " ['nansanhuan_aq', 'beijing_grid_303'],\n",
       " ['dongsihuan_aq', 'beijing_grid_324']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_join_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of Air Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of clusters : 2 , silhouette_score : 0.497\n",
      "No.of clusters : 3 , silhouette_score : 0.477\n",
      "No.of clusters : 4 , silhouette_score : 0.410\n",
      "No.of clusters : 5 , silhouette_score : 0.418\n",
      "No.of clusters : 6 , silhouette_score : 0.442\n",
      "No.of clusters : 7 , silhouette_score : 0.429\n",
      "No.of clusters : 8 , silhouette_score : 0.437\n",
      "No.of clusters : 9 , silhouette_score : 0.449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "    \n",
    "X = df_air_info[['long','lat']].values\n",
    "\n",
    "#cluster = DBSCAN(eps=0.24, min_samples=2).fit(X)\n",
    "cluster_result = []\n",
    "for n in range(2,10):\n",
    "    score = 0\n",
    "    for k in range(10):\n",
    "        cluster = KMeans(n_clusters=n, random_state=k).fit(X)\n",
    "        cluster_label = cluster.labels_\n",
    "        score += metrics.silhouette_score(X, cluster_label, metric='euclidean')/10\n",
    "    print(\"No.of clusters : %s\"  %n + \" , silhouette_score : %.3f\" %score)\n",
    "    cluster_result.append(score)\n",
    "    \n",
    "#Optimal No. of cluster = 6\n",
    "cluster = KMeans(n_clusters=6, random_state=10).fit(X)\n",
    "cluster_label = cluster.labels_\n",
    "clusters = []\n",
    "for i in set(cluster_label):\n",
    "    temp = [x[0] for j,x in enumerate(df_air_info.values) if cluster_label[j] == i]\n",
    "    clusters.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the cluster result to fill nan element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air_1804 = pd.read_csv('aiqQuality_201804.csv')\n",
    "df_air_1802_1803 = pd.read_csv('airQuality_201802-201803.csv')\n",
    "df_air_1701_1801 = pd.read_csv('airQuality_201701-201801.csv')\n",
    "\n",
    "df_air_1804.rename(index=str, columns={\"station_id\" : \"stationId\"\n",
    "                                       ,\"PM25_Concentration\": \"PM2.5\"\n",
    "                                       ,\"PM10_Concentration\": \"PM10\"\n",
    "                                      ,\"NO2_Concentration\" : \"NO2\"\n",
    "                                      ,\"CO_Concentration\" : \"CO\"\n",
    "                                      ,\"O3_Concentration\" : \"O3\"\n",
    "                                      ,\"SO2_Concentration\" : \"SO2\"}, inplace = True)\n",
    "\n",
    "df_air_1802_1803.rename(index=str, columns={\"utc_time\" : \"time\"}, inplace = True)\n",
    "df_air_1701_1801.rename(index=str, columns={\"utc_time\" : \"time\"}, inplace = True)\n",
    "\n",
    "df_air_1804.drop(['id'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juliana\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Juliana\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Juliana\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime , timedelta\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Here loading the file for each dataframe\n",
    "# df_air = df_air_1802_1803\n",
    "# df_air = df_air_1701_1801\n",
    "df_air = df_air_1804\n",
    "df_air['time'] = pd.to_datetime(df_air['time'])\n",
    "df_air['time'] = df_air['time'].astype(str)\n",
    "\n",
    "datetime_str_start = df_air['time'].min()\n",
    "datetime_str_end = df_air['time'].max()\n",
    "\n",
    "dt_start = datetime.strptime(datetime_str_start, \"%Y-%m-%d %H:%M:%S\")\n",
    "dt_end = datetime.strptime(datetime_str_end, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "time_length = int((dt_end - dt_start)/timedelta(hours = 1))\n",
    "\n",
    "for i in range(time_length + 1):\n",
    "    dt_pred = dt_start + timedelta(hours = i)\n",
    "    df_temp = df_air[(df_air['time'] == dt_pred.strftime(\"%Y-%m-%d %H:%M:%S\"))]\n",
    "    #fillna by their corresponding cluster\n",
    "    for k,cluster_list in enumerate(clusters):\n",
    "        df_temp_cluster = df_temp[df_temp['stationId'].isin(cluster_list)]\n",
    "        PM25_mean = df_temp_cluster['PM2.5'].mean()\n",
    "        PM10_mean = df_temp_cluster['PM10'].mean()\n",
    "        O3_mean = df_temp_cluster['O3'].mean()\n",
    "        #Error Detection for NaN value\n",
    "        if math.isnan(PM25_mean) == False:\n",
    "            df_temp_cluster['PM2.5'] = df_temp_cluster['PM2.5'].fillna(int(PM25_mean))\n",
    "        if math.isnan(PM10_mean) == False:\n",
    "            df_temp_cluster['PM10'] = df_temp_cluster['PM10'].fillna(int(PM10_mean))\n",
    "        if math.isnan(O3_mean) == False:\n",
    "            df_temp_cluster['O3'] = df_temp_cluster['O3'].fillna(int(O3_mean))\n",
    "        \n",
    "        if k == 0:\n",
    "            df_temp_fill = df_temp_cluster\n",
    "        else:\n",
    "            df_temp_fill = df_temp_fill.append(df_temp_cluster)\n",
    "            \n",
    "    PM25_mean = df_temp_fill['PM2.5'].mean()\n",
    "    PM10_mean = df_temp_fill['PM10'].mean()\n",
    "    O3_mean = df_temp_fill['O3'].mean()\n",
    "    #Error Detection for NaN value\n",
    "    if math.isnan(PM25_mean) == False:\n",
    "        df_temp_fill['PM2.5'] = df_temp_fill['PM2.5'].fillna(int(PM25_mean))\n",
    "    if math.isnan(PM10_mean) == False:\n",
    "        df_temp_fill['PM10'] = df_temp_fill['PM10'].fillna(int(PM10_mean))\n",
    "    if math.isnan(O3_mean) == False:\n",
    "        df_temp_fill['O3'] = df_temp_fill['O3'].fillna(int(O3_mean))\n",
    "    \n",
    "    if i == 0:\n",
    "        df_air_temp = df_temp_fill\n",
    "    else:\n",
    "        df_air_temp = df_air_temp.append(df_temp_fill)\n",
    "        \n",
    "df_air_temp = df_air_temp.sort_values(by=['stationId'])\n",
    "df_air_temp = df_air_temp.sort_values(by=['time'])\n",
    "df_air_temp = df_air_temp.fillna(method = 'ffill')\n",
    "df_air_temp = df_air_temp.fillna(method = 'bfill')\n",
    "\n",
    "#Save the file in csv format\n",
    "df_air_temp.to_csv('aiqQuality_201804(fillna).csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the neraset grid station to encode the weather attribute of the air station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#loading of every filled csv of air quality data\n",
    "df_air_1804 = pd.read_csv('aiqQuality_201804(fillna).csv')\n",
    "df_air_1802_1803 = pd.read_csv('airQuality_201802-201803(fillna).csv')\n",
    "df_air_1701_1801 = pd.read_csv('airQuality_201701-201801(fillna).csv')\n",
    "df_grid_1804 = pd.read_csv('gridWeather_201804.csv')\n",
    "df_grid_1701_1803 = pd.read_csv('gridWeather_201701-201803.csv')\n",
    "station_id = df_air_1804['stationId'].unique()\n",
    "\n",
    "df_grid_1804.drop(['id','weather'], axis = 1, inplace = True)\n",
    "df_grid_1701_1803.drop(['longitude', 'latitude'], axis = 1, inplace = True)\n",
    "\n",
    "df_grid_1701_1803.rename(index=str, columns={\"stationName\" : \"station_id\"\n",
    "                                             ,\"utc_time\" : \"time\"\n",
    "                                             ,\"wind_speed/kph\" : \"wind_speed\"}, inplace = True)\n",
    "\n",
    "storing_path = \"C:/Users/user/Desktop/MSBD5002 Forecast Project/station_data/\"\n",
    "if not os.path.exists(storing_path):\n",
    "    os.makedirs(storing_path)\n",
    "    \n",
    "for k in range(len(air_join_grid)):\n",
    "    df_1804 = df_air_1804[df_air_1804['stationId'] == air_join_grid[k][0]]\n",
    "    df_1802_1803 = df_air_1802_1803[df_air_1802_1803['stationId'] == air_join_grid[k][0]]\n",
    "    df_1701_1801 = df_air_1701_1801[df_air_1701_1801['stationId'] == air_join_grid[k][0]]\n",
    "    df_g_1701_1803 = df_grid_1701_1803[df_grid_1701_1803[\"station_id\"] == air_join_grid[k][1]]\n",
    "    df_g_1804 = df_grid_1804[df_grid_1804[\"station_id\"] == air_join_grid[k][1]]\n",
    "\n",
    "    frames_air = [df_1701_1801, df_1802_1803, df_1804]\n",
    "    frames_grid = [df_g_1701_1803, df_g_1804]\n",
    "    df_air = pd.concat(frames_air)\n",
    "    df_grid = pd.concat(frames_grid)\n",
    "    result = pd.merge(df_air, df_grid, how='left', on=['time'])\n",
    "    result.drop(['station_id','NO2','CO','SO2'], axis = 1, inplace = True)\n",
    "    result.dropna(inplace = True)\n",
    "    del df_air\n",
    "    del df_grid\n",
    "    result.to_csv(os.path.join(storing_path,air_join_grid[k][0] + '.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
